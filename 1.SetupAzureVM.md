# Setup Azure Data Science Virtual Machine

We can develop deep learning solution locally on our laptops but this needs lots of effort to setup development libraries and tools plus we will need a decent GPU to work quickly. The other option is to use the cloud. There are many options from all cloud providers but we will be using Azure data science virtual machine here. It's a virtual machine image provided from Microsoft with all the following pre-installed:

* NVIDIA libraries and drivers (The VM will have a GPU for sure)
* Deep learning libraries like TensorFlow, CNTK and Keras
* Jupyter notebooks and JupyterHub which allows remote notebook access.

I will be very brief here assuming the reader has some familiarity with Azure but the steps are really trivial.

## VM Creation
1. Using Azure portal, trigger create VM wizard & search for Data Science virtual machine using the Ubuntu template.
![data-science-vm](/images/data-science-vm.png)
1. During the creation wizard, pick **HDD** for the hard disk types as for some reason GPU VMs 
do not work with SSD hard disks or maybe not yet available.
1. For the location of the VM, pick `South Central US` as not all regions have GPU enabled VMs.
   This is not the only region with GPU enabled VMs but just to make things easy for you.
1. For the step of selecting VM size, choose NC6 or any other size you like provided it has one of the NVIDIA cards included.
![choose-vm-size](/images/choose-vm-size.png)
1. For VM SSH access, use username & password pair and remember them. Continue all remaining steps of the wizard and wait for the VM to be created.
 
## VM Preparation

Once the VM has been created a new window will be opened in Azure portal showing some details like the public IP of the created VM. We can RDP into it using tools like X2GO or whatever tool you prefer to use but we can do almost everything without a local GUI session.

First find out the public IP of the VM and try to hit the below in another browser tab:

`https://[VM public IP]:8000`

After bypassing a browser SSL warning, you will probably get the following:

![jupyter-hub-init](/images/jupyter-hub-init.png)

This means JupyterHub is ready to be started, click the Start button.

Once started, you will be navigated to login page in which you can use the same SSH credentials you used when creating the virtual machine.

![jupyter-hub-login](/images/jupyter-hub-login.png)


After login you will see Jupter home page

# TDOD: put jupter screenshot

Create a new folder called and rename it to Keras and then open that folder.
It will be empty and we will need to download a few notebooks and load them into it.

To Download the notebooks, create a new Python3 notebook and enter the following snippet into the first cell and execute it.

```
! wget https://raw.githubusercontent.com/ylashin/deep-learning-end-to-end/master/Notebooks/1.%2BDownload%2BData.ipynb

! wget https://raw.githubusercontent.com/ylashin/deep-learning-end-to-end/master/Notebooks/2.%2BFully%2Bconnected%2Bplain%2Bneural%2Bnetwork.ipynb

! wget https://raw.githubusercontent.com/ylashin/deep-learning-end-to-end/master/Notebooks/3.%2BAdd%2Bmore%2Bhidden%2Blayers.ipynb

! wget https://raw.githubusercontent.com/ylashin/deep-learning-end-to-end/master/Notebooks/4.%2BConvolution%2Bnetworks.ipynb
 
! wget https://raw.githubusercontent.com/ylashin/deep-learning-end-to-end/master/Notebooks/5.%2BDropout.ipynb

! wget https://raw.githubusercontent.com/ylashin/deep-learning-end-to-end/master/Notebooks/6.%2BTransfer%2BLearning.ipynb
```

The bang charater means run eveything after it as a bash command.

After executing the above snippet, Keras folder will have a few numbered notebooks we will follow in later steps. For now you can save of delete the current notebook, we are done with it.

Navigate back to Keras folder and proceed to [Progressively run Jupyter notebooks to come up with a trained model](2.RunJupyterNotebook.md).